(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{223:function(t,s,a){"use strict";a.r(s);var n=a(0),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"_2021科大讯飞鸟鸣识别比赛总结"}},[t._v("2021科大讯飞鸟鸣识别比赛总结")]),t._v(" "),a("p",[t._v("Yibo Liu, 2021")]),t._v(" "),a("h2",{attrs:{id:"比赛介绍"}},[t._v("比赛介绍")]),t._v(" "),a("p",[a("strong",[t._v("赛事链接:")]),t._v(" "),a("a",{attrs:{href:"https://challenge.xfyun.cn/topic/info?type=bird-call",target:"_blank",rel:"noopener noreferrer"}},[t._v("科大讯飞：鸟类鸣叫声识别挑战赛"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("strong",[t._v("任务描述:")]),t._v(" 鸟类鸣叫声识别挑战赛旨在增强自动鸟类鸣叫声识别技术，预测出每个测试音频中出现的鸟类物种。测试音频文件只包含单一的鸟类物种，预测在音频文件级别进行，不需要开始和结束的时间戳，属于单标签分类任务。")]),t._v(" "),a("p",[a("strong",[t._v("数据说明:")]),t._v(" 训练数据集包含100类鸟声数据，存在类别不均衡，真实背景噪音。不可以使用外部数据及预训练模型，不可以进行人工端点检测。")]),t._v(" "),a("h3",{attrs:{id:"相关比赛"}},[t._v("相关比赛")]),t._v(" "),a("ol",[a("li",[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/c/freesound-audio-tagging-2019",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle: Freesound Audio Tagging 2019"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("通用音频分类，多标签。")]),t._v(" "),a("p",[a("em",[t._v("The audio data is labeled using a vocabulary of 80 labels from Google’s AudioSet Ontology, covering diverse topics: Guitar and other Musical instruments, Percussion, Water, Digestive, Respiratory sounds, Human voice, Human locomotion, Hands, Human group actions, Insect, Domestic animals, Glass, Liquid, Motor vehicle (road), Mechanisms, Doors, and a variety of Domestic sounds.")])])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/c/birdsong-recognition",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle: Cornell Birdcall Identification"),a("OutboundLink")],1)])])]),t._v(" "),a("h3",{attrs:{id:"相关工作"}},[t._v("相关工作")]),t._v(" "),a("ol",[a("li",[a("p",[a("a",{attrs:{href:"https://github.com/lRomul/argus-freesound",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle: Freesound Audio Tagging 2019 第一名方案"),a("OutboundLink")],1),t._v("：有完整的音频预处理步骤和模型训练，使用了作者自己写的轻量级深度学习框架"),a("a",{attrs:{href:"https://github.com/lRomul/argus",target:"_blank",rel:"noopener noreferrer"}},[t._v("argus"),a("OutboundLink")],1),t._v("。本方案使用了基于CNN的模型，它的数据增强的作用很大，值得参考。该方案是我们的最终提交版本的baseline。")])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/daisukelab/creating-fat2019-preprocessed-data",target:"_blank",rel:"noopener noreferrer"}},[t._v("FAT 2019 数据预处理流程"),a("OutboundLink")],1),t._v("：上述第一名方案所参考的特征提取流程。")])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://github.com/ryanwongsa/kaggle-birdsong-recognition",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle: Cornell Birdcall Identification 第一名方案"),a("OutboundLink")],1),t._v("：使用了事件检测的流程，用了语音事件检测的预训练模型PANN。该比赛的任务似乎与本比赛不是很相符，因此没有采用。可参考"),a("a",{attrs:{href:"https://blog.csdn.net/GioDio/article/details/108673532?utm_medium=distribute.pc_relevant_download.none-task-blog-2~default~searchFromBaidu~default-10.test_version_3&depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-2~default~searchFromBaidu~default-10.test_version_",target:"_blank",rel:"noopener noreferrer"}},[t._v("相关博客"),a("OutboundLink")],1),t._v("。")])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://towardsdatascience.com/sound-based-bird-classification-965d0ecacb2b",target:"_blank",rel:"noopener noreferrer"}},[t._v("Poland Birdsong Classification"),a("OutboundLink")],1),t._v("：给出了一套数据处理流程，指出了数据现存的一些问题。")])])]),t._v(" "),a("h2",{attrs:{id:"观察数据"}},[t._v("观察数据")]),t._v(" "),a("ol",[a("li",[a("p",[a("strong",[t._v("类别比例")]),t._v("（数据包括train set和dev set）。坐标：类别-占比。")]),t._v(" "),a("p",[a("strong",[t._v("分析：")]),t._v(" 需要处理类别不平衡，考虑（1）重采样（2） Focal Loss。")]),t._v(" "),a("h4",{attrs:{id:""}})])]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/data_stat1.png",alt:"类别比例（包括train set和dev set）"}})]),t._v(" "),a("ol",{attrs:{start:"2"}},[a("li",[a("p",[a("strong",[t._v("音频长度")]),t._v("。坐标：采样点数-样本数。")]),t._v(" "),a("p",[a("strong",[t._v("分析：")]),t._v(" 需要考虑截取多长的时间片段作为输入。")])])]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/data_stat2.png",alt:"音频长度"}})]),t._v(" "),a("h2",{attrs:{id:"特征提取"}},[t._v("特征提取")]),t._v(" "),a("p",[t._v("语音的特征提取主要使用MFCC（梅尔倒谱系数），实际应用中发现使用log梅尔谱系数的情况也较多，同时通过比较两种方法得到的频谱图，本次比赛我们采用的是log梅尔谱系数。建议阅读参考资料 "),a("a",{attrs:{href:"https://www.cnblogs.com/luoqingyu/p/5929389.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("博客园 - 数字信号处理--傅里叶变换"),a("OutboundLink")],1),t._v(" 和 "),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/88625876",target:"_blank",rel:"noopener noreferrer"}},[t._v("知乎 - 语音识别第4讲：语音特征参数MFCC"),a("OutboundLink")],1),t._v("。")]),t._v(" "),a("h3",{attrs:{id:"mfcc"}},[t._v("MFCC")]),t._v(" "),a("h4",{attrs:{id:"设计原理"}},[t._v("设计原理")]),t._v(" "),a("p",[a("em",[t._v("根据人耳听觉机理的研究发现，人耳对不同频率的声波有不同的听觉敏感度。从200Hz到5000Hz的语音信号对语音的清晰度影响对大。两个响度不等的声音作用于人耳时，则响度较高的频率成分的存在会影响到对响度较低的频率成分的感受，使其变得不易察觉，这种现象称为掩蔽效应。由于频率较低的声音在内耳蜗基底膜上行波传递的距离大于频率较高的声音，故一般来说，低音容易掩蔽高音，而高音掩蔽低音较困难。在低频处的声音掩蔽的临界带宽较高频要小。所以，人们从低频到高频这一段频带内按临界带宽的大小由密到疏安排一组带通滤波器，对输入信号进行滤波。将每个带通滤波器输出的信号能量作为信号的基本特征，对此特征经过进一步处理后就可以作为语音的输入特征。")])]),t._v(" "),a("h4",{attrs:{id:"基本流程"}},[t._v("基本流程")]),t._v(" "),a("p",[t._v("连续语音 -> 预加重 ->分帧 -> 加窗 -> FFT -> Mel滤波器组 -> 对数运算 -> DCT（离散余弦变换）")]),t._v(" "),a("p",[t._v("（1）"),a("strong",[t._v("预加重")]),t._v("：增强高频部分，即通过一个高通滤波器。频域变换为 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("H")]),a("mo",[t._v("(")]),a("mi",[t._v("z")]),a("mo",[t._v(")")]),a("mo",[t._v("=")]),a("mn",[t._v("1")]),a("mo",[t._v("−")]),a("mi",[t._v("μ")]),a("msup",[a("mi",[t._v("z")]),a("mrow",[a("mo",[t._v("−")]),a("mn",[t._v("1")])],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("H(z)=1-\\mu z^{-1}")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.8141079999999999em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"1.064108em","vertical-align":"-0.25em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.04398em"}},[t._v("z")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mord mathrm"},[t._v("1")]),a("span",{staticClass:"mbin"},[t._v("−")]),a("span",{staticClass:"mord mathit"},[t._v("μ")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.04398em"}},[t._v("z")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord"},[t._v("−")]),a("span",{staticClass:"mord mathrm"},[t._v("1")])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("，对应的时域变换为"),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("y")]),a("mo",[t._v("(")]),a("mi",[t._v("t")]),a("mo",[t._v(")")]),a("mo",[t._v("=")]),a("mi",[t._v("x")]),a("mo",[t._v("(")]),a("mi",[t._v("t")]),a("mo",[t._v(")")]),a("mo",[t._v("−")]),a("mi",[t._v("α")]),a("mi",[t._v("x")]),a("mo",[t._v("(")]),a("mi",[t._v("t")]),a("mo",[t._v(")")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("y(t)=x(t)-\\alpha x(t)")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("t")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mord mathit"},[t._v("x")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("t")]),a("span",{staticClass:"mclose"},[t._v(")")]),a("span",{staticClass:"mbin"},[t._v("−")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mord mathit"},[t._v("x")]),a("span",{staticClass:"mopen"},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("t")]),a("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，这是实际计算时使用的。\n（2）"),a("strong",[t._v("分帧")]),t._v("：信号的频谱随时间变化，因此对整个信号进行傅立叶变换没有意义。假设频率在很短的时间内是平稳的，所以在短时间帧内进行傅里叶变换。\n（3）"),a("strong",[t._v("加窗")]),t._v("：以增加帧左端和右端的连续性。为了抵消FFT所假设的数据是无限的，并减少频谱泄漏。\n（4）"),a("strong",[t._v("FFT")]),t._v("：时域->频域\n（5）"),a("strong",[t._v("Mel滤波器组")]),t._v("：一系列滤波器，对不同频率设置不同的门限。\n（6）"),a("strong",[t._v("对数运算")]),t._v("\n（7）"),a("strong",[t._v("DCT（离散余弦变换）")]),t._v("：去除一些变化过快的系数，这些系数在ASR任务中没有帮助。")]),t._v(" "),a("h4",{attrs:{id:"尝试的几种实现"}},[t._v("尝试的几种实现")]),t._v(" "),a("ol",[a("li",[a("code",[t._v("librosa.feature.mfcc")])])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sample_rate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" librosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wav_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmfcc_feat "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" librosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mfcc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("sample_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_mfcc"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("ol",{attrs:{start:"2"}},[a("li",[a("code",[t._v("python_speech_features.mfcc")])])]),t._v(" "),a("p",[a("strong",[t._v("Note:")]),t._v(" 以上两种实现不包括预加重。")]),t._v(" "),a("ol",{attrs:{start:"3"}},[a("li",[t._v("手动实现")])]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" scipy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fftpack "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" dct\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("extract_mfcc_feature")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sample_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_mel_flt"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_ceps"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    original_signal"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("signal  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# signal[0:int(2.5*sample_rate)]")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pre emphasise")]),t._v("\n    pre_emphasis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.97")]),t._v("\n    emphasized_signal "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("original_signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" original_signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" pre_emphasis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" original_signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# frame")]),t._v("\n    frame_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.025")]),t._v("\n    frame_stride "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v("\n    frame_length "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("round")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("frame_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("sample_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    frame_step "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("round")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("frame_stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("sample_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n    signal_length "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("emphasized_signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    num_frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ceil"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("abs")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("signal_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("frame_length"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("frame_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pad_signal_length "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" frame_step "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" frame_length\n    pad_signal "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("emphasized_signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pad_signal_length "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" signal_length"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    indices "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("frame_length"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_frames"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tile"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("arange"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("num_frames"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("frame_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("frame_step"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("frame_length"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T\n    frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pad_signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mat"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("indices"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" copy"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# add window")]),t._v("\n    frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hamming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("frame_length"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# FFT")]),t._v("\n    NFFT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),t._v("\n    mag_frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("absolute"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fft"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rfft"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("frames"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" NFFT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Magnitude of the FFT")]),t._v("\n    pow_frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" NFFT"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mag_frames "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 功率谱")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Mel filter -> fbanks")]),t._v("\n    low_freq_mel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    nfilt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" n_mel_flt\n    high_freq_mel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2595")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log10"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sample_rate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("700")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    mel_points "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linspace"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("low_freq_mel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" high_freq_mel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nfilt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Equally spaced in Mel scale")]),t._v("\n    hz_points "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("700")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mel_points "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2595")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Convert Mel to Hz")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("floor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("NFFT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" hz_points "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" sample_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    fbank "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nfilt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("floor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("NFFT "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" m "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nfilt "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        f_m_minus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# left")]),t._v("\n        f_m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("             "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# center")]),t._v("\n        f_m_plus "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# right")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f_m_minus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" f_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            fbank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("k "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("f_m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" f_m_plus"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            fbank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("bin")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("m"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    filter_banks "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dot"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pow_frames"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fbank"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("T"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    filter_banks "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("where"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filter_banks "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("finfo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("eps"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filter_banks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Numerical Stability")]),t._v("\n    filter_banks "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log10"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filter_banks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dB")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# DCT -> mfcc")]),t._v("\n    num_ceps "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" n_ceps\n    mfcc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dct"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filter_banks"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" norm"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ortho'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_ceps "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nframes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ncoeff"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" mfcc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n    \n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# normalize mfcc")]),t._v("\n    mfcc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mfcc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" mfcc  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (n_frames, n_features)")]),t._v("\n")])])]),a("h3",{attrs:{id:"logmelspec"}},[t._v("LogMelSpec")]),t._v(" "),a("p",[t._v("LogMelSpec与MFCC区别在于它没有DCT。可用的实现是"),a("code",[t._v("librosa.features.melspectrogram")]),t._v("。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sample_rate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" librosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("wav_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmelspec "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" librosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("feature"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("melspectrogram"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sr"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("sample_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_fft"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("hop_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("n_mels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlogmelspec "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" librosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("power_to_db"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("melspec"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("h3",{attrs:{id:"不同提取特征方式的对比"}},[t._v("不同提取特征方式的对比")]),t._v(" "),a("p",[t._v("我们希望通过观察特征图像来判断那种特征更合适。参与对比的有mfcc的两种实现和logmelspec，他们的特征数量（即梅尔滤波器数量）均设置为128，实验代码详见"),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/preprocessing/feature_extract.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v("。通过对比，我们最终选用logmelspec作为特征。")]),t._v(" "),a("p",[a("strong",[t._v("wav时域图：")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/wav_img.png",alt:"wav_img"}})]),t._v(" "),a("p",[a("strong",[t._v("librosa.features.mfcc:")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/mfcc.png",alt:"mfcc"}})]),t._v(" "),a("p",[a("strong",[t._v("python_speech_features:")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/python_speech.png",alt:"python_speech"}})]),t._v(" "),a("p",[a("strong",[t._v("librosa.features.logmelspec:")])]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/logmelspec.png",alt:"logmelspec"}})]),t._v(" "),a("h2",{attrs:{id:"数据增强"}},[t._v("数据增强")]),t._v(" "),a("p",[t._v("目前对语音信号的建模方式为：对整段音频信号提取频谱图，将频谱图视为图像，沿该图像的时间轴取一小段定长片段，放入处理图像的模型中，如CNN等。因此数据增强的对象是提取的特征图像，数据增强会包括截取、加噪等。特征的形状：dim_x=整个音频的帧数，dim_y=特征数量(滤波器个数)。")]),t._v(" "),a("p",[t._v("我们主要参考"),a("a",{attrs:{href:"https://github.com/lRomul/argus-freesound#augmentations",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle: Freesound Audio Tagging 2019 第一名方案"),a("OutboundLink")],1),t._v("，所使用的数据增强包括以下步骤：")]),t._v(" "),a("ol",[a("li",[a("p",[a("strong",[t._v("截取片段")])]),t._v(" "),a("p",[t._v("由于鸟鸣在一段音频中的出现是周期性的，而数据集中音频的长度不一，因此对每段音频***随机***截取一小段（256帧）作为训练数据。长度不够256帧的要补齐。这一步有许多之的改进的地方：")]),t._v(" "),a("p",[t._v("（1）具体帧数可以更改，256帧是照搬的freesound方案，应该统计我们数据中每一声鸟鸣的长度是多少，截取的长度要综合考虑鸟鸣长度和鸟鸣间隔。")]),t._v(" "),a("p",[t._v("（2）不一定要随机截取，可以辅以端点检测和静音检测（题目中说不可以使用人工端点检测，但我们可以使用自动的），以避免取到不包含鸟鸣的片段。")]),t._v(" "),a("p",[t._v("（3）进而引发的思考是，截取片段的方法只用到了局部信息，没有用到全局信息（如鸟鸣间隔，鸟一共名叫多少声等等），因此后续我们尝试了序列模型（CNN特征提取器+LSTM后端）来利用全局信息。")]),t._v(" "),a("p",[t._v("% TODO： 加图")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("随机缩放")])]),t._v(" "),a("p",[t._v("freesound方案作者称该步骤提升显著，我们照搬了。")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("加噪")])]),t._v(" "),a("p",[t._v("我认为有两种思路，一种是给图像加噪，即在特征图像上加矩形/sin函数形的mask；另一种是对原始音频添加白噪声/粉噪声等。")]),t._v(" "),a("p",[t._v("我们截至比赛截至时只尝试了freesound使用的矩形mask和我们增加的sin函数形mask。（但提交版本暂未加入sin函数形mask，也尚未实验验证其效果。）图像加噪的效果如下：")]),t._v(" "),a("p",[t._v("% TODO：add sin")]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/mask.png",alt:"mask"}})]),t._v(" "),a("p",[t._v("原始音频加噪的方法可以参考"),a("a",{attrs:{href:"https://github.com/ryanwongsa/kaggle-birdsong-recognition#data-augmentation",target:"_blank",rel:"noopener noreferrer"}},[t._v("Cornell Birdcall Identification 第一名方案的数据增强"),a("OutboundLink")],1),t._v("， 我个人认为这种加噪方法更符合直觉，值得尝试。")])])]),t._v(" "),a("p",[a("strong",[t._v("其他尝试：预加重")])]),t._v(" "),a("p",[t._v("预加重是在提取特征之前对原始音频的处理，增强高频部分。librosa的logmelspec特征并没有预加重的步骤，因此我们手动添加。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("sample_rate "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" librosa"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fname"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npre_emphasis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.97")]),t._v("\npre_emphasised_signal "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" pre_emphasis "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" signal"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("增加预加重后valid acc提升了1.89%（0.71->0.7389）。")]),t._v(" "),a("h2",{attrs:{id:"模型"}},[t._v("模型")]),t._v(" "),a("ul",[a("li",[t._v("Vision Transformer")]),t._v(" "),a("li",[t._v("CNN")]),t._v(" "),a("li",[t._v("CNN特征提取+序列模型（LSTM/Transformer）")])]),t._v(" "),a("h3",{attrs:{id:"vision-transformer"}},[t._v("Vision Transformer")]),t._v(" "),a("p",[t._v("考虑使用Vision Transformer的动机是，Kaggle Freesound和Cornell Birdcall都是2019或2020年的比赛，而Transformer是2019年底才流行起来、Vision Transformer更是2020年才出现，所以以前这些比赛获奖者所采用的CNN方案未必是当下最好的方案。因此我们决定直接上强有力的模型，Vision Transformer。对与分类任务，有"),a("a",{attrs:{href:"https://arxiv.org/abs/2010.11929",target:"_blank",rel:"noopener noreferrer"}},[t._v("ViT"),a("OutboundLink")],1),t._v("和CvT。")]),t._v(" "),a("p",[a("strong",[t._v("Vision Transformer原理讲解：")]),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/348593638",target:"_blank",rel:"noopener noreferrer"}},[t._v("知乎 - Vision Transformer , Vision MLP超详细解读 (原理分析+代码解读)"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("strong",[t._v("可用的开源实现：")])]),t._v(" "),a("ol",[a("li",[a("p",[a("a",{attrs:{href:"https://github.com/rwightman/pytorch-image-models",target:"_blank",rel:"noopener noreferrer"}},[t._v("timm"),a("OutboundLink")],1),t._v("：一个包含各种视觉模型的库。使用方法可以参考"),a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/350837279",target:"_blank",rel:"noopener noreferrer"}},[t._v("知乎 - 视觉Transformer优秀开源工作：timm库vision transformer代码解读"),a("OutboundLink")],1),t._v("。官方介绍："),a("em",[t._v("PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more")]),t._v("。")])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://github.com/lucidrains/vit-pytorch",target:"_blank",rel:"noopener noreferrer"}},[t._v("vit-pytorch"),a("OutboundLink")],1),t._v("：一个包含各种Vision Transformer的库。")])])]),t._v(" "),a("h4",{attrs:{id:"验证性实验"}},[t._v("验证性实验")]),t._v(" "),a("p",[t._v("为了测试ViT的可用性，先在"),a("a",{attrs:{href:"https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle猫狗分类数据集"),a("OutboundLink")],1),t._v("上跑了ViT，准确率为~0.6，这并不高。在我们的Bird 4k数据集上（为方便调试取了一个小数据集，大小为4k，由每类别随机取等数量的样本得到）上得到的准确率为~0.02。")]),t._v(" "),a("p",[t._v("为了测试CvT的可用性，先在通用图像分类数据集"),a("a",{attrs:{href:"http://www.vision.caltech.edu/Image_Datasets/Caltech256/",target:"_blank",rel:"noopener noreferrer"}},[t._v("caltech256"),a("OutboundLink")],1),t._v("上跑了CvT，准确率为~0.15。")]),t._v(" "),a("h4",{attrs:{id:"预训练模型"}},[t._v("预训练模型")]),t._v(" "),a("p",[t._v("Transformer是在使用了预训练后才大放异彩（BERT），因此这里我们也考虑预训练。因为没有找到现成的Vision Transformer在图像或者在语音数据上的预训练模型，我们打算自己写预训练。能想到的预训练的方法有两种：")]),t._v(" "),a("ol",[a("li",[a("p",[a("strong",[t._v("自编码器")]),t._v("。这是非常符合直觉的，但可能不好训练，因为要恢复的内容太多，CvT可能会很复杂。")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("预测Mask的任务")]),t._v("（像BERT那样）。训练起来可能会更容易（因为预测的内容只是局部），但关键是设计的预测任务要确保合理有效、能帮到后面的分类任务。")])])]),t._v(" "),a("p",[t._v("首先我们为CvT设计了自编码器：")]),t._v(" "),a("p",[a("img",{attrs:{src:"/article/bird_song/cvt_ptr.png",alt:"cvt_ptr"}})]),t._v(" "),a("p",[t._v("（上面是预训练模型，下面是分类模型）")]),t._v(" "),a("p",[t._v("自编码器的解码器目前使用了一个直觉设计的CNN，这里有修改的空间。")]),t._v(" "),a("p",[a("strong",[t._v("实验结果：")])]),t._v(" "),a("p",[t._v("对于caltech256，预训练使用的数据是其训练集，使用了预训练后分类准确率没有提高。可能的原因是预训练的数据不够多，Transformer这种复杂模型很容易在小数据集上过拟合。")]),t._v(" "),a("p",[t._v("对于Bird 4k数据集，预训练使用的数据也是4k数据集，无预训练分类准确率~0.02，有预训练分类准确率~0.076，这说明预训练是有效的。训练过程中训练集准确率达0.9以上，说明过拟合了。")]),t._v(" "),a("p",[a("strong",[t._v("关于Vision Transformer预训练模型的结论：")])]),t._v(" "),a("p",[t._v("预训练并非是无效的。实验中观察到预训练模型loss的下降非常缓慢，这可能是由于需要大量数据和时间，而我们数据不够大、也仅跑了最多几百个epoch来观察。BERT的训练花费了3天3夜，起初的效果也并不显著。")]),t._v(" "),a("p",[t._v("如不使用预训练则效果比不过CNN，如想要预训练有效则需要大量数据和时间。该预训练模型的改进方向是：（1）搜集更多外部数据（鸟鸣的，或至少是用于音频分类的）（2）修改自编码器的Decoder结构。于是我们暂别Transformer，转投CNN的怀抱。")]),t._v(" "),a("h3",{attrs:{id:"cnn"}},[t._v("CNN")]),t._v(" "),a("p",[t._v("从这里开始，我们使用完整数据集训练。在训练Vision Transformer时，为了节约时间，我们使用的是一个大小为4k子数据集。")]),t._v(" "),a("h4",{attrs:{id:"simple-cnn"}},[t._v("Simple CNN")]),t._v(" "),a("p",[t._v("首先尝试了一个简单的CNN并使用了batchnorm（"),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/train_script/model_SimpleCNN_dataAug_Bird.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v("），在完整数据集上未使用数据增强得到的结果是valid acc=0.58，使用数据增强得到的结果是valid acc=0.68。此外，也尝试了ResNet101和ResNet50，均未达到更好效果。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Classifier")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            ConvBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (B,64,64,128)")]),t._v("\n            ConvBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (B,128,32,64)")]),t._v("\n            ConvBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (B,256,16,32)")]),t._v("\n            ConvBlock"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# (B,512,8,16)")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PReLU"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BatchNorm1d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Linear"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("       "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[B,512,8,16]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("avg_pool2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[B,512,4,8]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#x,_ = x.max(dim=-1)    #[B,512,4,1]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rearrange"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B C H W -> B C (H W)'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[B,512,32]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[B,512,1]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[B,512]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fc"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("         "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[B,num_class]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x\n")])])]),a("h4",{attrs:{id:"改进的cnn：auxskipattn"}},[t._v("改进的CNN：AuxSkipAttn")]),t._v(" "),a("p",[t._v("进而尝试了freesound方案的模型AuxSkipAttn（"),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/train_script/model_AuxSkipAttn_dataAug_Bird.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v("）作为baseline，简言之它是一个添加了Attention，Skip connection和Auxiliary classifier的CNN，得到的结果是valid acc=0.72，提交测试结果test acc=0.64。这个结果说明测试数据和训练数据分布不一致，因此训练数据不平衡的问题需要引起重视并解决。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_skip_attention "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AuxSkipAttention\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Classifier")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_skip_attn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AuxSkipAttention"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("base_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dropout"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("ratio"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("last_filters"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("last_fc"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_skip_attn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux1\n")])])]),a("h4",{attrs:{id:"预训练模型-2"}},[t._v("预训练模型")]),t._v(" "),a("p",[t._v("图像领域CNN的预训练模型已经广泛使用，因次希望音频特征图也能用上。首先，挪用通用图像分类的预训练模型如ResNet等是不可取的，因为图像的差异太大。其次，我们尝试寻找通用的音频特征图的CNN预训练模型，未果。因此我们决定自己训一个鸟鸣音频的预训练模型。")]),t._v(" "),a("p",[t._v("为了扩大训练数据，我们搜集了许多外部数据集，包括：")]),t._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/c/freesound-audio-tagging-2019/data",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle Freesound Auto Tagging 2019 dataset"),a("OutboundLink")],1)])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/rtatman/british-birdsong-dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("British Birdsong Dataset"),a("OutboundLink")],1)])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/c/birdsong-recognition/data",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle: Cornell Birdcall Identification dataset"),a("OutboundLink")],1)])])]),t._v(" "),a("p",[t._v("这次我们采用了将预测Mask作为任务的预训练方案("),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/train_script/model_MaskPretrain_dataAug_Bird.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v(")。mask的长度取为32。")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ConvDecoder")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("input_dim"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("outsz_h"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("outsz_w"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ConvDecoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv1d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("input_dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv3 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv4 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv5 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv6 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Conv2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("in_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("out_channels"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("stride"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropout "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("batchnorm "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dim"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [4, 1, 1024, 1024]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [4, 3, 512, 512]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [4, 3, 256, 256]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv4"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [4, 3, 128, 128]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv5"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [4, 3, 64, 64]")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("F"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_pool2d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("conv6"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[4, 3*4, 32, 32]     ")]),t._v("\n        x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rearrange"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B (c1 c2) H W -> B c2 (c1 H) W'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c1"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#[4,4*128,32]  ->  [4,3,128,32]")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" models"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_skip_attention "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" AuxSkipAttention\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("PreTrainer")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Module"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("hid_dim"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mask_h"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mask_w"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_skip_attn "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" AuxSkipAttention"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("hid_dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("base_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("dropout"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("ratio"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("last_filters"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("last_fc"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_weights "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ConvDecoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("input_dim"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("hid_dim"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("outsz_h"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mask_h"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("outsz_w"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("mask_w"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("forward")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        hid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aux1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("aux_skip_attn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# hid=(batch_size,1024)")]),t._v("\n        hid "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dim"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decoder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hid"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n")])])]),a("p",[a("strong",[t._v("实验结果：")]),t._v(" 预训练loss下降非常缓慢，训练30epoch后停止，分类器valid acc=0.7。可能的原因：预训练模型没有训好。可改进的点：（1）ConvDecoder可以更好地设计（这里只是凭直觉设计的）（2）预训练使用的4个数据集的数据量为42011，原始数据量为10906，可能仍不够多。")]),t._v(" "),a("h4",{attrs:{id:"规整化隐空间"}},[t._v("规整化隐空间")]),t._v(" "),a("p",[t._v("希望各类别的数据能符合高斯混合分布，因此需要对隐空间做一个变换来使之符合这种分布。我们分别尝试了：")]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("Deep Normalize Flow (DNF)")]),t._v(" ("),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/train_script/model_AuxSkipAttn_DNF_dataAug_Bird.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v(")")]),t._v(" "),a("p",[t._v("Flow模型是一种生成式模型，这里利用了其归一化的原理。")]),t._v(" "),a("p",[a("em",[t._v("A flow-based generative model is a generative model used in machine learning that explicitly models a probability distribution by leveraging normalizing flow,[1] which is a statistical method using the change-of-variable law of probabilities to transform a simple distribution into a complex one. -- "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Flow-based_generative_model",target:"_blank",rel:"noopener noreferrer"}},[t._v("Wikipedia"),a("OutboundLink")],1)])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("Wasserstein Auto-Encoder (WAE)")]),t._v(" ("),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/train_script/model_AuxSkipAttn_WLoss_dataAug_Bird.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v(")")])])]),t._v(" "),a("h3",{attrs:{id:"cnn特征提取-序列模型（lstm-transformer）"}},[t._v("CNN特征提取+序列模型（LSTM/Transformer）")]),t._v(" "),a("p",[t._v("思路是将将整段特征图像切分为若干帧长为128的片段（此前是整段特征图像随机取一个片段，此处要输入全部片段），将AuxAttnSkip作为编码器为每一个128帧长度的片段编码，将编码后的特征序列输入LSTM或Transformer，就像输入embedding后的文本序列。这样做的目的是利用全局信息，而不只是局部信息。该方法有待尝试。")]),t._v(" "),a("h3",{attrs:{id:"模型集成"}},[t._v("模型集成")]),t._v(" "),a("p",[t._v("根据许多人的比赛经验，bagging/boosting/stacking能显著提高效果。这里我们仅仅是将同一个模型以不同的随机种子在测试集上预测5或10次，并投票取结果，就获得了test acc从0.64到0.72的提升（该模型的valid acc=0.72）。详见"),a("a",{attrs:{href:"https://github.com/zll17/BirdRec/blob/main/train_script/pred_AuxSkipAttn_Bird.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),a("OutboundLink")],1),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"致谢"}},[t._v("致谢")]),t._v(" "),a("p",[t._v("感谢队友"),a("a",{attrs:{href:"https://github.com/zll17",target:"_blank",rel:"noopener noreferrer"}},[t._v("@方阿"),a("OutboundLink")],1),t._v("的鼎力支持，期待下一次！ 😃")])])}),[],!1,null,null,null);s.default=e.exports}}]);